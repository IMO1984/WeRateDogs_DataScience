{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling report on WeRateDogs Tweeter Archive\n",
    "\n",
    "   \n",
    "   \n",
    "   >***Author:*** *Indranil Mondal*    \n",
    "   >***Date:*** *25th Sept, 2022*\n",
    "\n",
    "### Introduction\n",
    "Data collected manually or electronically most often comes with dirty data. The process of gathering data from a multiple in multiple formats, assessing its quality and tidiness, finally cleaning it is called data wrangling. In this project the goal is to apply concepts learned to analyse Twitter data of WeRateDogs and produce trustworthy insight with some degree of visualization.\n",
    "\n",
    "### Project Detail\n",
    "Here in this project, I will be wrangling the tweet archive of WeRateDogs. Requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in gathered dataset. \n",
    "\n",
    "Udacity partially gathered data for us. Udacity extracted rating, dog name, and dog \"stage\" and created enhanced twitter archive in csv format.Additionally, Udacity curated image prediction dataset by extracting all images from twitter archive and applying _Neural Network_ alogorithm on them to predict Dog's breed in tsv format. I imported these datasets uding panda library. Moreover, to get the retweet and favorite count from Twitter I used Tweety API to fetch tweet status data.\n",
    "\n",
    "Next come assessing the gathered data. I had to look for two types of unclean data namely - **Dirty data** and **untidy data**. Untidy data has **structural issues**, whereas Dirty data has content issue. I used both **Visual** and **Programmatic** assements to find data issues. Findings from Visual assements included both Tidiness issues(e.g Dog stages values are considered as Columns, Rating Column distributed over two) as well as quality issues (e.g Dog Names, Predicted Breed names does not conform to consistent Pattern, Wrong Dog names, Predicted Dog breed is not a dog breed). Programmatic assessment revealed quite a number of data qulity issue too. \n",
    "\n",
    "Cleaning of the data based on findings from **Assesment** are done in this step. For each and every findings, I repeated Defining, Coding and testing. Worthwhile to mention, Cleaning up one issue at times resolved another data quality issues and sometimes I found it is intertwined between **Tidiness** and **Data Quality issue**. Solving one solved other. At time I also found one is dependent upon another. In order to address Tidiness issue I had to first address Quality issue. This steps made me go through different panda functions to explore. Main learnings from this steps are how to renaming a cloumn, changing datatypes of columns, Manipulating strings in panda, droping rows/ columns handling Nan/null. Most importantly, I kind of developed an eye and intuition on the data and how to approch it.\n",
    "\n",
    "The cleaned up data could habe been stored as different dataset, but I felt and was also suggested by Udacity store them in a single dtaset by combining all three cleaned up datasets into a master. This enabled me to analyze the data efficiently and with relative ease I would say. I had to join all three dataframes on common column i.e **tweet_id** to create the twitter_archive_master.csv. Please note evethough we started with a lot more data in all three of them, after cleaning and joining them together I have a lot less data(1532) but with good quality.\n",
    "\n",
    "In this step I had to analyse the clean consolidated data and make meaningfull insights. I spent hours to figure what possibly could I infer. I had to revisit how twitter schema works to find out about re-tweet, favourite and certain shortcomings on my knowledge on how twitter work. Finally I made few interesting insights, which I certainly think are correct ones. I made a big assumption here that neural network algorithm did its job prety well. I manually verified few of the prediction by looking at the images and was satisfied by the predictions.\n",
    "\n",
    "### Conclusion\n",
    "When I started this project, I thought it would be peice of cake and soon realize how wrong I was.It was quite journey thorugh the project - and I leanred quite a lot of new things. I hope to use them in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
